}
beta_optimize <- matrix(NA,max_iter,p)
for (iter in 1:max_iter){
if (iter == 1){
b_old <- b_new <- rep(0,p)
beta_optimize[,1] <- b_old
}
for (j in 1:p){
if (abs(pg_Huber_reg(b_old[-j],n,y,x[,-j],w,delta)) <= lambda){
b_new[j] <- 0
next
}else{
rj <- (y - x[,-j]%*%b_old[-j])/x[,j]
b_new[j] <- optimize(f=fun_pen_Huber_reg_fixed,c(b_old[j]-10*delta,b_old[j]+10*delta),
n=n,r=rj,x=x[,j],w=w,delta=delta,lambda=lambda)$minimum
}
}
print(round(b_new,3))
beta_optimize[iter,] <- b_new
if (norm(b_new-b_old,"2") < tol){
print("converged!, from normal condition check")
beta_optimize <- beta_optimize[1:iter,]
return(list(beta_hat = beta_optimize, iter = iter))
}else if (iter >= 3){
if( norm(beta_optimize[iter,] - beta_optimize[(iter-2),],"2") < tol){
# print("converged!, from oscillating condition check")
beta_optimize <- beta_optimize[1:iter,]
return(list(beta_hat = beta_optimize, iter = iter))
}
}else{
b_old <- b_new
}
}
}
# --------------------------------------------------------------- #
# penalized Huber regression:
fun_pen_Huber_reg <- function(beta,n,y,x,w,delta,lambda){
if (is.null(w)){
w <- rep(1,n)
}
sum(mapply(i=1:n,function(i) fun_L(w[i]*(y[i] - x[i,] %*% beta),delta)))/n + lambda*sum(abs(beta))
}
# partial gradient of penalized Huber regression:
pg_Huber_reg <- function(beta,n,y,x,w,delta){
if (is.null(w)){
w <- rep(1,n)
}
sum(mapply(i=1:n,function(i) fun_dL(w[i]*(y[i] - x[i,] %*% beta),delta)))/n
}
# Huber loss:
fun_L <- function(v, delta){
if (abs(v) <= delta){
return(v^2/2)
}else{
return(delta*abs(v) - delta^2/2)
}
}
# Huber loss gradient:
fun_dL <- function(v, delta){
if (abs(v) <= delta){
return(v)
}else{
return(delta*sign(v))
}
}
#
# # --------------------------------------------------------------- #
# # Set simulation
# set.seed(123)
# lambda <- 0.05;
# # lambda <- 1;
# delta <- 1.5
#
# n <- 100
# p <- 20
# x <- matrix(rnorm(n*p,0,1),n,p)
# e <- rnorm(n,0,1)
# # w <- runif(n,min=0,max=1)
# w <- NULL
#
# b_star <- c(10,-10,rep(0,p-2))
# y <- x %*% b_star + e
#
# max_iter <- 100; tol <- 1e-8
#
# # 1. coordinate descent:
# fit_cd <- ftn_cd(y,x,w,delta,lambda,max_iter,tol)
# beta_cd <- fit_cd$beta_hat[fit_cd$iter,]
# round(beta_cd,3)
# round(fun_pen_Huber_reg(beta_cd,n,y,x,w,delta,lambda),3)
#
# # 2. brute force:
# fit_bf <- ftn_bf(y,x,w,delta,lambda,max_iter,tol)
# beta_bf <- fit_bf$beta_hat[fit_bf$iter,]
# round(beta_bf,3)
# round(fun_pen_Huber_reg(beta_bf,n,y,x,w,delta,lambda),3)
#
# # 3. Use optimizer (optimize):
# fit_optimize <- ftn_optimize(y,x,w,delta,lambda,max_iter,tol)
# beta_optimize <- fit_optimize$beta_hat[fit_optimize$iter,]
# round(beta_optimize,3)
# round(fun_pen_Huber_reg(beta_optimize,n,y,x,w,delta,lambda),3)
#
fun_cd(x[,j],w,rj,n,delta,lambda)
.C("cd_Huber",
double(1),
double(1),
as.double(x[,j]),
as.double(w),
as.double(rj),
as.integer(n),
as.double(delta),
as.double(lambda))
.C("cd_Huber",
as.double(1),
double(1),
as.double(x[,j]),
as.double(w),
as.double(rj),
as.integer(n),
as.double(delta),
as.double(lambda))
fun_cd(x[,j],w,rj,n,delta,lambda)
.C("cd_Huber",
as.double(1),
double(1),
as.double(x[,j]),
as.double(w),
as.double(rj),
as.integer(n),
as.double(delta),
as.double(lambda))
set.seed(123)
lambda <- 0.05;
delta <- 1.5
n <- 100
p <- 10
x <- matrix(rnorm(n*p,0,1),n,p)
e <- rnorm(n,0,1)
b <- c(10,-10,rep(0,p-2))
y <- x %*% b + e
max.iter=1e6
tol=1e-7
w=NULL
dyn.load("D:/High-dimensional time series/Exact coordinate descent/rome/scr/huber_test.dll")
iter <- 1
j <- 1
n <- dim(x)[1]
p <- dim(x)[2]
if (is.null(w)){
w <- rep(1,n)
}
beta.cd <- matrix(NA,max.iter,p)
if (iter == 1){
b.old <- b.new <- rep(0,p)
beta.cd[1,] <- b.old
}
rj <- (y - x[,-j]%*%b.old[-j])/x[,j]
.C("cd_Huber",
double(1),
double(1),
as.double(x[,j]),
as.double(w),
as.double(rj),
as.integer(n),
as.double(delta),
as.double(lambda))
rj
w
rbind(rj-delta,rj+delta)
sort(rbind(rj-delta,rj+delta))
sort(rbind(rj-delta,rj+delta))[100]
sort(rbind(rj-delta,rj+delta))[99]
sort(rbind(rj-delta,rj+delta))[98]
sort(rbind(rj-delta/abs(x[,j]/n),rj+delta/abs(x[,j]/n)))
sort(rbind(rj-delta/abs(x[,j]/n),rj+delta/abs(x[,j]/n)))[100]
sort(rbind(rj-delta/abs(x[,j]),rj+delta/abs(x[,j])))[100]
set.seed(123)
lambda <- 0.05;
delta <- 1.5
n <- 100
p <- 10
x <- matrix(rnorm(n*p,0,1),n,p)
e <- rnorm(n,0,1)
b <- c(10,-10,rep(0,p-2))
y <- x %*% b + e
max.iter=1e6
tol=1e-7
w=NULL
dyn.load("D:/High-dimensional time series/Exact coordinate descent/rome/scr/huber_test.dll")
iter <- 1
j <- 1
n <- dim(x)[1]
p <- dim(x)[2]
if (is.null(w)){
w <- rep(1,n)
}
beta.cd <- matrix(NA,max.iter,p)
if (iter == 1){
b.old <- b.new <- rep(0,p)
beta.cd[1,] <- b.old
}
rj <- (y - x[,-j]%*%b.old[-j])/x[,j]
.C("cd_Huber",
double(1),
double(1),
as.double(x[,j]),
as.double(w),
as.double(rj),
as.integer(n),
as.double(delta),
as.double(lambda))
fun_cd(x[,j],w,rj,n,delta,lambda)
#
# # solve the problem with p>2 and add KKT conditions.
# # --------------------------------------------------------------- #
# rm(list=ls())
# library(latex2exp)
# library(superheat)
# library(rgl)
# --------------------------------------------------------------- #
# Coordinate descent, multivariate:
ftn_cd <- function(y,x,w,delta,lambda,max_iter,tol){
n <- dim(x)[1]
p <- dim(x)[2]
if (is.null(w)){
w <- rep(1,n)
}
beta_cd <- matrix(NA,max_iter,p)
for (iter in 1:max_iter){
if (iter == 1){
b_old <- b_new <- rep(0,p)
beta_cd[1,] <- b_old
}
for (j in 1:p){
if (abs(pg_Huber_reg(b_old[-j],n,y,x[,-j],w,delta)) <= lambda){
b_new[j] <- 0
next
}else{
rj <- (y - x[,-j]%*%b_old[-j])/x[,j]
b_new[j] <- fun_cd(x[,j],w,rj,n,delta,lambda)
}
}
print(round(b_new,3))
beta_cd[iter,] <- b_new
if (norm(b_new-b_old,"2") < tol){
print("converged!, from normal condition check")
beta_cd <- beta_cd[1:iter,]
return(list(beta_hat = beta_optimize, iter = iter))
}else if (iter >= 3){
if( norm(beta_cd[iter,] - beta_cd[(iter-2),],"2") < tol){
# print("converged!, from oscillating condition check")
beta_cd <- beta_cd[1:iter,]
return(list(beta_hat = beta_cd, iter = iter))
}
}else{
b_old <- b_new
}
}
}
# Exact coordinate descent for Huber, univariate:
fun_cd <- function(x,w,r,n,delta,lambda){
thres <- delta/abs(w*x)
dat <- array(NA, c(2*n, 6))
colnames(dat) <- c("r_i", "i", "r pm delta/|x_j|", "B", "Slope", "f'(c)")
dat[,1] <- c(r,r) # r_i.
dat[,2] <- c(rep(1:n, 2)) # indices i of r_i.
dat[,3] <- c(r-thres, r+thres) # kinks; r_i pm r pm delta/|x_j|.
dat[,4] <- c(w^2*x^2/n,-w^2*x^2/n) # increment when passing the kinks with respect to c.
dat <- dat[order(dat[,3]),] # rearrange data in the order of kinks.
dat[,5] <- cumsum(dat[,4]) # This tell us about the slope.
S <- -delta*sum(abs(w*x))/n
dat[1,6] <- S + ifelse(dat[1,3] >= 0, lambda, 0) + ifelse(dat[1,3] <= 0, -lambda,0)
for (i in 1:(nrow(dat)-1)){
S <- S + dat[i,5]*(dat[i+1,3]-dat[i,3])
dat[i+1,6] <- S + ifelse(dat[(i+1),3] >= 0, lambda, 0) + ifelse(dat[(i+1),3] <= 0, -lambda,0)
if (i>1){
if ((dat[(i-1),6] <0) & (dat[i,6] >= 0)){
x1 <- dat[i-1,3];
x2 <- dat[i,3]
y0 <- dat[i-1,6];
y1 <- dat[i,6]
out <- x1 - y0*(x2-x1)/(y1-y0)
}
}
}
return(out)
}
# --------------------------------------------------------------- #
# Brute force, multivariate:
ftn_bf <- function(y,x,w,delta,lambda,max_iter,tol){
n <- dim(x)[1]
p <- dim(x)[2]
if (is.null(w)){
w <- rep(1,n)
}
beta_bf <- matrix(NA,max_iter,p)
for (iter in 1:max_iter){
if (iter == 1){
b_old <- b_new <- rep(0,p)
beta_bf[,1] <- b_old
}
for (j in 1:p){
if (abs(pg_Huber_reg(b_old[-j],n,y,x[,-j],w,delta)) <= lambda){
b_new[j] <- 0
next
}else{
rj <- (y - x[,-j]%*%b_old[-j])/x[,j]
b_new[j] <- fun_brute(x[,j],w,rj,n,delta,lambda)
}
}
print(round(b_new,3))
beta_bf[iter,] <- b_new
if (norm(b_new-b_old,"2") < tol){
print("converged!, from normal condition check")
beta_bf <- beta_bf[1:iter,]
return(list(beta_hat = beta_bf, iter = iter))
}else if (iter >= 3){
if( norm(beta_bf[iter,] - beta_bf[(iter-2),],"2") < tol){
# print("converged!, from oscillating condition check")
beta_bf <- beta_bf[1:iter,]
return(list(beta_hat = beta_bf, iter = iter))
}
}else{
b_old <- b_new
}
}
}
# Brute force for Huber, univariate:
fun_brute <- function(x,w,r,n,delta,lambda){
thres <- delta/abs(w*x)
cseq <- seq(sort(r-thres)[1], sort(r+thres)[n], length.out=100*n)
data_oracle <- array(NA, c(length(cseq), 3))
colnames(data_oracle) <- c("c", "f(c)", "f'(c)")
data_oracle[,1] <- cseq
for (i in 1:nrow(data_oracle)){
tmp1 <- 0; tmp2 <- 0
for (j in 1:n){
tmp1 <- tmp1 + fun_L(w[j]*x[j]*(cseq[i] - r[j]), delta)/n
tmp2 <- tmp2 + fun_dL(w[j]*x[j]*(cseq[i] - r[j]), delta)/n
}
data_oracle[i,2:3] <- c((tmp1 + lambda*abs(cseq[i])),
(tmp2+ifelse(cseq[i]>=0, lambda, 0)+ifelse(cseq[i]<=0, -lambda, 0)))
}
j <- which.min(data_oracle[,2])
return(data_oracle[j,1])
}
# --------------------------------------------------------------- #
# Use optimizer (optimize):
ftn_optimize <- function(y,x,w,delta,lambda,max_iter,tol){
n <- dim(x)[1]
p <- dim(x)[2]
if (is.null(w)){
w <- rep(1,n)
}
fun_pen_Huber_reg_fixed <- function(beta,n,r,x,w,delta,lambda){
sum(mapply(i=1:n,function(i) fun_L(w[i]*x[i]*(r[i]-beta),delta)))/n + lambda*sum(abs(beta))
}
beta_optimize <- matrix(NA,max_iter,p)
for (iter in 1:max_iter){
if (iter == 1){
b_old <- b_new <- rep(0,p)
beta_optimize[,1] <- b_old
}
for (j in 1:p){
if (abs(pg_Huber_reg(b_old[-j],n,y,x[,-j],w,delta)) <= lambda){
b_new[j] <- 0
next
}else{
rj <- (y - x[,-j]%*%b_old[-j])/x[,j]
b_new[j] <- optimize(f=fun_pen_Huber_reg_fixed,c(b_old[j]-10*delta,b_old[j]+10*delta),
n=n,r=rj,x=x[,j],w=w,delta=delta,lambda=lambda)$minimum
}
}
print(round(b_new,3))
beta_optimize[iter,] <- b_new
if (norm(b_new-b_old,"2") < tol){
print("converged!, from normal condition check")
beta_optimize <- beta_optimize[1:iter,]
return(list(beta_hat = beta_optimize, iter = iter))
}else if (iter >= 3){
if( norm(beta_optimize[iter,] - beta_optimize[(iter-2),],"2") < tol){
# print("converged!, from oscillating condition check")
beta_optimize <- beta_optimize[1:iter,]
return(list(beta_hat = beta_optimize, iter = iter))
}
}else{
b_old <- b_new
}
}
}
# --------------------------------------------------------------- #
# penalized Huber regression:
fun_pen_Huber_reg <- function(beta,n,y,x,w,delta,lambda){
if (is.null(w)){
w <- rep(1,n)
}
sum(mapply(i=1:n,function(i) fun_L(w[i]*(y[i] - x[i,] %*% beta),delta)))/n + lambda*sum(abs(beta))
}
# partial gradient of penalized Huber regression:
pg_Huber_reg <- function(beta,n,y,x,w,delta){
if (is.null(w)){
w <- rep(1,n)
}
sum(mapply(i=1:n,function(i) fun_dL(w[i]*(y[i] - x[i,] %*% beta),delta)))/n
}
# Huber loss:
fun_L <- function(v, delta){
if (abs(v) <= delta){
return(v^2/2)
}else{
return(delta*abs(v) - delta^2/2)
}
}
# Huber loss gradient:
fun_dL <- function(v, delta){
if (abs(v) <= delta){
return(v)
}else{
return(delta*sign(v))
}
}
#
# # --------------------------------------------------------------- #
# # Set simulation
# set.seed(123)
# lambda <- 0.05;
# # lambda <- 1;
# delta <- 1.5
#
# n <- 100
# p <- 20
# x <- matrix(rnorm(n*p,0,1),n,p)
# e <- rnorm(n,0,1)
# # w <- runif(n,min=0,max=1)
# w <- NULL
#
# b_star <- c(10,-10,rep(0,p-2))
# y <- x %*% b_star + e
#
# max_iter <- 100; tol <- 1e-8
#
# # 1. coordinate descent:
# fit_cd <- ftn_cd(y,x,w,delta,lambda,max_iter,tol)
# beta_cd <- fit_cd$beta_hat[fit_cd$iter,]
# round(beta_cd,3)
# round(fun_pen_Huber_reg(beta_cd,n,y,x,w,delta,lambda),3)
#
# # 2. brute force:
# fit_bf <- ftn_bf(y,x,w,delta,lambda,max_iter,tol)
# beta_bf <- fit_bf$beta_hat[fit_bf$iter,]
# round(beta_bf,3)
# round(fun_pen_Huber_reg(beta_bf,n,y,x,w,delta,lambda),3)
#
# # 3. Use optimizer (optimize):
# fit_optimize <- ftn_optimize(y,x,w,delta,lambda,max_iter,tol)
# beta_optimize <- fit_optimize$beta_hat[fit_optimize$iter,]
# round(beta_optimize,3)
# round(fun_pen_Huber_reg(beta_optimize,n,y,x,w,delta,lambda),3)
#
fun_cd(x[,j],w,rj,n,delta,lambda)
thres <- delta/abs(w*x[,j])
dat <- array(NA, c(2*n, 6))
colnames(dat) <- c("r_i", "i", "r pm delta/|x_j|", "B", "Slope", "f'(c)")
dat[,1] <- c(rj,rj) # r_i.
dat[,2] <- c(rep(1:n, 2)) # indices i of r_i.
dat[,3] <- c(rj-thres, rj+thres) # kinks; r_i pm r pm delta/|x_j|.
dat[,4] <- c(w^2*x[,j]^2/n,-w^2*x[,j]^2/n) # increment when passing the kinks with respect to c.
dat <- dat[order(dat[,3]),] # rearrange data in the order of kinks.
dat[,5] <- cumsum(dat[,4]) # This tell us about the slope.
dat
.C("cd_Huber",
as.double(1),
as.double(1),
as.double(x[,j]),
as.double(w),
as.double(rj),
as.integer(n),
as.double(delta),
as.double(lambda))
.C("cd_Huber",
double(1),
double(1),
as.double(x[,j]),
as.double(w),
as.double(rj),
as.integer(n),
as.double(delta),
as.double(lambda))
.C("cd_Huber",
double(1),
double(1),
as.double(x[,j]),
as.double(w),
as.double(rj),
as.integer(n),
as.double(delta),
as.double(lambda))
.C("cd_Huber",
double(1),
double(1),
as.double(x[,j]),
as.double(w),
as.double(rj),
as.integer(n),
as.double(delta),
as.double(lambda))
library(glmnet)
library(hqreg)
?hqreg
?glmnet
?.C
